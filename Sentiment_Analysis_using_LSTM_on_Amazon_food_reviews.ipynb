{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis using LSTM on Amazon food reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGSWOUFpEwwCMofDJElBKv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PearlSikka/language-ninja/blob/master/Sentiment_Analysis_using_LSTM_on_Amazon_food_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VZ22TlsFCYr"
      },
      "source": [
        "The project is to understand the process of classifying sentiments from reviews of fine foods from amazon using Long Short Term Memory networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlMt-EnUlmNN"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Upload the API token.\n",
        "def get_kaggle():\n",
        "  try:\n",
        "    import kaggle\n",
        "    return kaggle\n",
        "  except OSError:\n",
        "    pass\n",
        "\n",
        "  token_file = pathlib.Path(\"~/.kaggle/kaggle.json\").expanduser()\n",
        "  token_file.parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "  try:\n",
        "    from google.colab import files\n",
        "  except ImportError:\n",
        "    raise ValueError(\"Could not find kaggle token.\")\n",
        "\n",
        "  uploaded = files.upload()\n",
        "  token_content = uploaded.get('kaggle.json', None)\n",
        "  if token_content:\n",
        "    token_file.write_bytes(token_content)\n",
        "    token_file.chmod(0o600)\n",
        "  else:\n",
        "    raise ValueError('Need a file named \"kaggle.json\"')\n",
        "  \n",
        "  import kaggle\n",
        "  return kaggle\n",
        "\n",
        "\n",
        "kaggle = get_kaggle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oktHP2o2ldIn"
      },
      "source": [
        "!kaggle datasets download -d snap/amazon-fine-food-reviews                          #downloading kaggle dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHPnj_9NE77N"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilS35B9ZmAs9"
      },
      "source": [
        "!unzip amazon-fine-food-reviews.zip -d train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi_DfK4mmIAq"
      },
      "source": [
        "#loading libraries \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense,SpatialDropout1D,LSTM,Embedding\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVilFMPSmTA-"
      },
      "source": [
        "data=pd.read_csv('/content/train/Reviews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olllJSXhmYER"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezKIWIUCucSM"
      },
      "source": [
        "data.describe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yWyj1y3uhyN"
      },
      "source": [
        "data=data[['Text','Score']]       #filtering columns Text, Score "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37K3Vcc-u0B1"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSeMMlFKu1fi"
      },
      "source": [
        "data=data[data.Score !=3]    #removing neutral reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pVa1CUVvRPr"
      },
      "source": [
        "data.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3V6ld6Kva8I"
      },
      "source": [
        "rename_dict={1:0,2:0,4:1,5:1}                                                        #reviews having score 1, 2 -> negative, 4,5->positive "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJglAW9dzmkH"
      },
      "source": [
        "rename_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5mfmbyz0BQ_"
      },
      "source": [
        "data =data.replace({\"Score\": rename_dict})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lcR7ky5GDv-"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMUwKww7Mtg5"
      },
      "source": [
        "stemmer=PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsb3AkRwOJ9D"
      },
      "source": [
        "def clean_word(text):                                                       \n",
        "  split_sent= text.split()\n",
        "  cleaned_Word=\" \".join(stemmer.stem(i) for i in split_sent if                              #stemming the words\n",
        "                        i not in stopwords.words('english'))\n",
        "  return cleaned_Word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npL8DcpGSFJ1"
      },
      "source": [
        "data1=data[data['Score']==0][:4000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdKZjEjA0Z4r"
      },
      "source": [
        "data1=data1.append(data[data['Score']==1][:4000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za4j_AsUc-cq"
      },
      "source": [
        "pos_cnt=data1[data1.Score==1]\n",
        "neg_cnt=data1[data1.Score==0]\n",
        "\n",
        "print(pos_cnt.shape)\n",
        "print(neg_cnt.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EirFpmtMxf9"
      },
      "source": [
        "tokenizer=Tokenizer(num_words=1000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=' ')       #tokenizer to tokenize the sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QR0mXwXJ2sP"
      },
      "source": [
        "tokenizer.fit_on_texts(data1['Text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuGkOGFTKGMA"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8O085i2JaQO"
      },
      "source": [
        "print(len(tokenizer.word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QmAFtOWKTjm"
      },
      "source": [
        "sequences=tokenizer.texts_to_sequences(data1['Text'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVP1bFd0K77j"
      },
      "source": [
        "print(sequences[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMsl8OonLC3D"
      },
      "source": [
        "padded=pad_sequences(sequences,maxlen=40,padding='post')                          #post padding sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXbg2oTBLiWZ"
      },
      "source": [
        "print(padded[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMoo9wG_SxkV"
      },
      "source": [
        "X=padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK-q9Up5Luc6"
      },
      "source": [
        "embed_dim = 10\n",
        "lstm_out = 100\n",
        "\n",
        "model = Sequential()                                                             #sequential model    \n",
        "model.add(Embedding(1000, embed_dim,input_length = X.shape[1]))                  #Embedding layer     \n",
        "model.add(SpatialDropout1D(0.4))                                                 #Dropout layer\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))                    #LSTM layer\n",
        "model.add(Dense(1,activation='softmax'))                                          \n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',               \n",
        "              metrics = ['accuracy'])   \n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUxMx3ksTHfw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhsNN8isUM7d"
      },
      "source": [
        "Y=data1['Score'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w64BKPnrUR5g"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6bzrp3uWMae"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGmzqtVJT1Aq"
      },
      "source": [
        "#train_X,train_Y,test_X,test_Y= train_test_split(X,Y,test_size=0.25)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytGdqXvyUxsX"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umtz-UIOT1ib"
      },
      "source": [
        "model.fit(X_train,Y_train,batch_size=32,epochs=4)                                #training model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbNmakugVIKX"
      },
      "source": [
        "validation_size = 1000                                                           #validating model\n",
        "\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = 32)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VopkTD9IVI9T"
      },
      "source": [
        "# twt = ['not tasty']\n",
        "# twt = tokenizer.texts_to_sequences(twt)\n",
        "# twt = pad_sequences(twt, 40, dtype='int32', value=0)\n",
        "# sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
        "# print(int(sentiment))\n",
        "\n",
        "# #print(np.argmax(sentiment))\n",
        "\n",
        "# if((sentiment) == 0):\n",
        "#     print(\"negative\")\n",
        "# elif ((sentiment) == 1):\n",
        "#     print(\"positive\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}